
```{r 'check_ps', include=FALSE}

user.name = 'ENTER A USER NAME HERE'
```


# God Insures Those Who Pay?

Hello there! Welcome to the problem set "God Insures Those Who Pay?". This is a part of my master thesis at Ulm Univeristy, and is based on the paper [God Insures Those Who Pay? Formal Insurance and Religious Offerings in Ghana][link1] by Emmanuelle Auriol, Julie Lassébie, Amma Panin, Eva Raiber, and Paul Seabright. The paper was published in *The Quarterly Journal of Economics* in 2020. The dataset can be downloaded from [here][link2].  

[link1]: https://academic.oup.com/qje/article/135/4/1799/5861944#206772545
[link2]: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/KVFRFO


## Exercise 1. Introduction

When sudden and unforeseen financial shocks decide to give us a surprise visit, it provides a little relief to have insurance against it. But what if you are living your life, as uncertain as it is, without being formally insured against the risks that might come knocking on your door?  

Whom do you rely on then?  


*God?*  
  
  
The paper looks at the role of religious institutions in providing informal insurance and tries to answer the question *whether religious believers offer donations in the hope of receiving insurance against economic shocks*. And if it is the case, then to what extent do they expect God and the church to intervene?  

To answer this, the authors study the effects of providing formal market-based insurance when there is a demand for informal church-based insurance. They do so by conducting a randomized experiment with members of a Pentecostal Church in Accra, Ghana. In this experiment the members of the church were randomly assigned into one of the following three groups:    

* Insurance  

* Insurance Information  

* No Insurance  

The members assigned to the **Insurance** group were then enrolled into a funeral insurance policy. This group corresponds to the *insurance enrollment treatment*. Funerals are large and costly events in many sub-Saharan African societies *(Case et al. 2013; Berg 2018)* and the surviving family members are expected to honor the dead with lavish commemorations, thereby making a funeral insurance policy attractive in this setting *(paper, p. 1812)*.  

The members assigned to the **Insurance Information** group were explained about the insurance policy but were *not* enrolled in it. This group corresponds to the *control group*.  

The members assigned to the **No Insurance** group were neither explained about the insurance, nor enrolled in it. This group corresponds to the *no insurance information treatment*.  


After assigning the members to the different groups, they were made to play a series of dictator games where they had to allocate money between themselves and a set of religious goods. It was found that the members enrolled in insurance gave *significantly less* money to their church and other recipients than the members who only received information about insurance, thereby indicating that insurance might be one of the motives for religious participation. We will see this in more detail in the next sections.  

Before we start, let's take a look at what this problem set contains.  

### Problem Set - Content:

1. **Introduction**  

2. **Dictator Games**  

3. **Descriptive Overview**   

  3.1 Data Summary  

  3.2 Randomization Checks  

  3.3 Plotting variables across treatments
  
  3.4 Plotting results of dictator games

4. **Insurance enrolment and Insurance information**   

  4.1 OLS Regression  

  4.2 OLS Regression with Control Variables  

  4.3 Tobit Regression  

5. **Conclusion**  

6. **References**  


Throughout the problem set you will encounter some quizzes, R-exercises, and lots of awards! To start solving an exercise, click on the `edit` button, and to check your solution, click on the `check` button. If you get stuck, don't worry, you can get some hints by clicking on the `hint` button. Note that it is not necessary to go through the sections in the given order but I highly recommend you to do so.  

Now that you have an idea of what lies ahead, let's get started!  

$~$

## Exercise 2. Dictator Games

The dictator game is a popular game studied in experimental economics. In a dictator game, there are typically two parties involved: the 'proposer' or 'sender', and the 'receiver'. The proposer is given some endowment and s/he can decide how much, if any, to allocate to the receiver; the proposer can keep the entire endowment for themselves, or give some (or all) of it to the receiver. So, the proposer *dictates* what the receiver gets, hence the name of the game.  

Before we talk about the dictator games played in the experiment, let's play a dictator game with you! 

$~$

<iframe src="https://docs.google.com/forms/d/e/1FAIpQLSeZvYne1l6J9bLt7FLtaB0YudQWoaIPkuDndlIqVTFOWAknAw/viewform?embedded=true" width="700" height="520" frameborder="0" marginheight="0" marginwidth="0">Loading…</iframe>

$~$

Do you think there is perhaps some pattern? Maybe the proportion given away is more when the recipient is a charity? Or maybe the proportion given away is less when the endowment amount is more? Or perhaps people usually split it 50-50?


The assumptions of traditional economic theories suggest that people will always make the most self-interested choice, according to which the dictator should always end up choosing to keep the entire endowment for themselves. However, behavioral experiments find that although some people do indeed give nothing, usually majority of the people behave altruistically and give something. In a meta-study of dictator games by Christoph Engel it was found that 36.11% of all participants gave nothing to the recipient, 16.74% chose the equal split, and as many as 5.44% gave the recipient everything (Engel, Christoph, [*Dictator Games: A Meta Study*][link_meta_study], p. 7). Behavioral economics is quite an interesting subject and if you are interested in learning more about it then Cartwright, Edward, *Behavioral Economics* (Third ed., 2018) might prove to be a good starting point.

[link_meta_study]: https://homepage.coll.mpg.de/pdf_dat/2010_07online.pdf

Coming back to the paper, the participants of the experiment played 10 games and the order of the games was randomized. The participants were paid a flat show-up fee of GHS 20. For each game the participants were asked to allocate GHS 11 between two recipients, and after all the games were played, one of the games was selected at random for each participant, and he or she was paid further according to the decision taken for this game. Thus, the participants had the opportunity to earn up to GHS 31. (3.8 GHS was equal to 1 USD in 2015.)

The pairs of the recipients for the 10 games are listed in the table below:

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("preknit_nZaAGMKyFKHf")
```

The first four games in the above table are classical dictator games where the participant had to split the endowment between herself/himself and the other party, whereas the other 6 games are a modification in which the participant had to split the endowment between two different recipients (other than himself/herself).  

Church refers to the church of the participant. Giving to church can be seen as using the church as an insurer in a number of ways: the individuals might give to church expecting the church to reward them by disbursing funds in times of need, or they might also give to church to portray that they are good community members to other church members, hoping that they would then contribute to help them in times of need. These type of community-based insurance is considered as *material* insurance.
Again, it is important to note that there is no claim that this is the *only motive* or even the *main motive* of the people who are giving to church.  

The Street Children's Fund is a secular charity that takes care of the education needs of homeless and vulnerable children. Giving to this charity is interpreted as an altruistic action.   

The Thanksgiving offering is a part of a nationwide interfaith prayer event in which leaders and members of various faiths join together in prayer for Ghana. Giving towards this event is interpreted as giving towards a largely spiritual interest.  

The secular and spiritual consumption can be thought of as *spiritual* insurance where the individuals give to church and secular and spiritual charities due to their belief in Divine intervention, i.e. belief in an interventionist God who would lower the probability of loss, or reduce the severity of shocks.

***

For the rest of the paper, we will mainly focus on three main dictator games:  
* Self vs. Church (anonymous)  
* Self vs. Street Children (anonymous)  
* Self vs. Thanksgiving (anonymous)  

So, unless otherwise stated, whenever we talk about the 'results of the dictator games' we will be referring to these three games.

$~$

## Exercise 3.1 Data Summary

We will use the data `wave1_and_wave2_church_data.csv` for the next few sections. Before we can start working with the data, we need to first load it.  

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Loading data using read.csv()")
```

**Exercise:** Run the code below by pressing `check`.

```{r "3_3"}
# Load wave1_and_wave2_church_data.csv into the variable dat using command read.csv()
dat <- read.csv("wave1_and_wave2_church_data.csv")
```

Let's have a look at what this dataset contains.  

**Exercise:** Use the command `head()` to display the first 6 rows of `dat`.

```{r "3_4"}
# Use head() to look at the first 6 rows of dat 
# Enter your code below:
```

Hmm, we seem to have a lot of variables. The command `ncol()` returns the number of columns the dataset contains. Take a guess at the command that returns the number of rows of the dataset.  

**Exercise:** Calculate the number of rows and columns in the dataset `dat`.
  
```{r "3_5"}
# Use ncol() to get the number of columns of the dataset dat
# Enter your code below:

# Get the number of rows of the dataset
# Enter your code below:

```

Since we will not be using all of the variables, let us first keep only the ones that are relevant for the next few sections. To do this, we will use the command `select()` from the `dplyr` package.  

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Loading Packages in R")
```


```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("dplyr - select(), mutate(), filter()")
```


```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Pipe Operator %>%")
```


**Exercise:** Simply run the code chunk below by clicking on `edit` and then press `check`.

```{r "3_11"}
# First load the package dplyr
library(dplyr)

# Selecting the relevant variables
dat_2 <- dat %>% 
  select(treatment, age, gender, ethnicity, higheducation, employed, 
         tmonthlyincome, church_daily, church_weekly, pray_multiple, 
         travels_more_than_hour, revival, choice_keep_church, 
         choice_keep_thanks, choice_keep_street, wave, church_name_raw, 
         church_name_grouped, Tinsurance, Tinsuranceinfo, Tnoinsurance)
```



Let's take a look at the dataset again. This time we will use the command `sample_n()` to display a random sample of our dataset. The number of rows you want to be displayed can be specified using the second parameter `size`.  

**Exercise:** Use the `sample_n()` command to display 5 random rows of the dataset `dat_2`.

```{r "3_12"}
# Use sample_n() to see 5 rows of the dataset dat_2
# Enter your code below:
```

The variable `treatment` contains information about the group the participant was assigned to. Recall that the three possible groups are: 'Insurance' (which corresponds to the treatment enrollment into the insurance policy), 'Insurance information' (which corresponds to the control group receiving information about insurance but not getting enrolled in the policy), and 'No insurance' (which corresponds to the treatment group that neither received information about insurance nor were they enrolled in the policy).

The variables `choice_keep_church`, `choice_keep_thanks`, and `choice_keep_street`, are the results of the dictator games 'Self vs. Church (anonymous)', 'Self vs. Thanksgiving (anonymous)', and 'Self vs. Street Children's Fund (anonymous)' respectively. The value of the variable `choice_keep_church` indicates the proportion of endowment that the participant gave to church, and 1 minus this value is the proportion that the participant kept with themselves. Similarly, `choice_keep_thanks` and `choice_keep_street` represent the proportion of endowment that the participant gave to thanksgiving prayer event, and Street Children's Fund, respectively.

A complete list of variables of the dataset `dat_2` along with their definitions can be found below. Please go through this before proceeding further.  

* `treatment` - Group the participant was assigned to. Possible values: 'Insurance', 'Insurance information', or 'No insurance'.  

* `age` - Age of the participant.  

* `gender` - Gender of the participant.  

* `ethnicity` - Ethnicity of the participant. Possible values: Akan, Ewe, Ga Adangbe.  

* `higheducation` - '1' if participant had high school education, '0' otherwise.  

* `employed` - Employment status of the participant. '1' if participant was employed when the experiment took place, '0' if not.  

* `tmonthlyincome` - Monthly income of the participant when the experiment took place.  

* `church_daily` - '1' if participant goes to church daily, '0' if not.  

* `church_weekly` - '1' if participant goes to church weekly, '0' if not.  

* `pray_multiple` - '1' if participant prays multiple times a day, '0' if not.  

* `travels_more_than_hour` - '1' if participant travels more than an hour to arrive at church, '0' otherwise.  

* `revival` - '1' if participant attended the experiment during revival weeks, '0' otherwise. Revival weeks are intense periods of church-going and fund-raising during which congregations typically give substantially more to church than normal (*paper*, p. 1802).  
  
* `choice_keep_church` - Result of dictator game 'Self vs. Church (anonymous)'; value shows the proportion of endowment the participant gave to church (anonymously), and remaining proportion kept for themselves.  

* `choice_keep_thanks` - Result of dictator game 'Self vs Thanks (anonymous)'; value shows the proportion of endowment the participant gave to thanksgiving prayer event (anonymously), and remaining proportion kept for themselves.  

* `choice_keep_street` - result of dictator game 'Self vs Street (anonymous)'; value shows the proportion of endowment the participant gave to Street Children's Fund (anonymously), and remaining proportion kept for themselves.

* `wave` - The experiment was conducted in two waves; the first wave was done in Nov-Dec 2016 and the second wave was done in May-June 2019.  

* `church_name_raw` - The name of the church of the participant.  

* `church_name_grouped` - `church_name_raw` was grouped into 12 groups.  

* `Tinsurance` - '1' if participant was assigned to 'Insurance' treatment group, '0' otherwise.  

* `Tinsuranceinfo` - '1' if participant was assigned to 'Insurance information' group, '0' otherwise.  

* `Tnoinsurance` - '1' if participant was assigned to 'No insurance' treatment group, '0' otherwise.  


Now that we know what the variables contain, let's go one step further and look at the *class* of the variable. One way to do this is by using the command `str()`. An alternative is the command `glimpse()`. They make it possible to see every column of the data, and next to the column name we can see the class of the respective variable.  

**Exercise:** Call the command `str()` on the dataset `dat_2`.
```{r "3_13"}
# Call str() on dat_2
# Enter your code below:

```

Observe that we have variables of type *chr*, *int*, and *num*.  

* **Character**: These are variables that contain values as strings.  

* **Integer**: As the name suggests, these are variables that contain integer values.   

* **Numeric**: The variable type numeric or double can hold numeric values with decimal points.  

Let us introduce one more important type of variable: *factor*.  

* **Factor**: Factors are variable types that are used to categorize the data and store it as levels. They can store both strings and integers. For example, our variable `treatment` contains only three possible values: 'Insurance', 'Insurance information', and 'No insurance'. Instead of keeping it as a character variable, we can convert it to the type factor, which will recognize that there are only three values that are getting repeated, and these three values are then stored as *levels*. This is particularly helpful when we want to get the summary statistic of the variable, or while modeling. It also takes up less storage space as compared to characters.  


We will now change the type of classes for some of the variables.  

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("mutate_if() and mutate_at()")
```


**Exercise:** Go through the below code and run it.
```{r "3_14"}
# Changing variable types
dat_2 <- dat_2 %>% 
  # converting all character variables to factors
  mutate_if(is.character, as.factor) %>% 
  # converting some other variables to factor
  mutate_at(vars("higheducation", "employed", "church_daily", "church_weekly",
                 "pray_multiple", "travels_more_than_hour", "revival", 
                 "wave", "Tinsurance", "Tinsuranceinfo", "Tnoinsurance"),
            as.factor)
```


Now make some guesses about the data.

***


Quiz: What do you think about the gender distribution of the dataset?

[1]: I think the dataset has more males than females.
[2]: I think the dataset has more females than males.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("gender quiz")
```

***


Quiz: Do you think majority of the participants of the dataset go to church daily?

[1]: Yes.
[2]: No.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("church daily quiz")
```

***


Quiz: Do you think majority of the participants of the dataset go to church weekly?

[1]: Yes.
[2]: No.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("church weekly quiz")
```

***

Instead of making guesswork about the data, wouldn't it be much more useful if we could get its summary and see what it actually contains? Well, our wish is granted! One way to get the summary of the dataset is by using the command `summary()`, and an elegant alternative is the `skim()` command from the `skimr` package. We will use the latter.   

**Exercise:** Load the `skimr` package and then call the command `skim()` on the dataset `dat_2`.

<style>
div#chunk_3_15_chunkUI td{
  padding-left: 1em;
  padding-right: 1em
}
div#chunk_3_15_chunkUI th{
  padding-left: 1em;
  padding-right: 1em
}
</style>
```{r "3_15"}
# Load the package skimr
# Enter your code below:


# Call skim() on dat_2
# Enter your code below:

```

***

In the output, you first get an overview of the number of rows and columns of the dataset, and the types of variables that your dataset contains. Then you can see an overview of the variables separated by the variable types.

For the factor variables, the first two columns give information about the missing data for the respective variable: `n_missing` contains the number of missing observations, and `complete_rate` gives you the proportion of complete information. The column `n_unique` tells you the number of levels of that variable, and in the column `top_counts` you get further details about these levels.

For the variables of type numeric, you again have information about missing data in the columns `n_missing` and `complete_rate`. The columns `mean` and `sd` give the mean and standard deviation, respectively. You can then find information about the percentiles: column `p0` contains the minimum value, `p25` gives the $25^{th}$ percentile, `p50` is the median or the $50^{th}$ percentile, `p75` contains the $75^{th}$ percentile and `p100` shows the maximum value. The column `hist` displays the histogram of that variable enabling you to have a rough idea about how the variable is distributed.

***

As compared to the national population, the participants of this experiment had relatively lower incomes and were less likely to be employed. However, this was not considered to be a problem as the aim of the experiment was particularly to know the attitude towards insurance of precisely this subgroup of the general population who are likely more vulnerable and face a greater number of formally uninsured risks.

$~$

That's all for this section! In the next section we will perform some randomization checks.

$~$

## Exercise 3.2 Randomization Checks

As discussed earlier, the authors had conducted a randomized experiment and divided the participants into three treatment groups: Insurance enrolment, Insurance information, and No insurance. But how can we check if the randomized experiment was actually *random*? That is, how do we know if gender, or monthly income, or other features were *balanced* across the different treatments? Of course it is possible that the imbalance, for example one group having more unemployed than employed participants than the other groups, was purely out of chance, but was the imbalance significantly large? The imbalance becomes especially of interest if the imbalanced variable has an impact on (or is correlated with) our main variable of interest (in our case, the proportions of endowment given to the different entities).

This is where balance tests (also known as randomization checks) can be used - they help in checking how good the randomization was, and if there were any significant imbalances.  

Before we proceed, we need to load the dataset and make the necessary changes again.

**Exercise:** Replace the ___ with appropriate commands.

```{r "4_1"}
# Load the dataset
dat <- ___("wave1_and_wave2_church_data.csv")

# Make necessary changes in dataset dat and store it in variable dat_2
dat_2 <- ___ %>% 
  # selecting desired variables
  ___(treatment, age, gender, ethnicity, ethnicity_akan, 
         ethnicity_ewe, ethnicity_ga, higheducation,
         employed, tmonthlyincome, church_daily, church_weekly,
         pray_multiple, travels_more_than_hour, revival, 
         choice_keep_church, choice_keep_thanks, choice_keep_street, 
         wave, Tinsurance, Tinsuranceinfo, Tnoinsurance) %>% 
  # creating dummy variables for gender and wave
  mutate(Female = ifelse(gender == "Female", 1, 0),
         Wave_2 = ifelse(wave == 2, 1, 0)) %>% 
  # converting all character variables to factors
  ___(is.character, as.factor) %>% 
  # converting some other variables to factor
  ___(vars("higheducation", "employed", "church_daily", "church_weekly",
                 "pray_multiple", "travels_more_than_hour", "revival", 
                 "wave", "Tinsurance", "Tinsuranceinfo", "Tnoinsurance"),
            as.factor)
```


Let's start by checking whether age is balanced across the different treatments. Intuitively, one way to do this is to calculate the average age for each treatment group and check if they are significantly different. This can be achieved by using Welch's two-sample t-test. However, as we can only use two samples for Welch's t-test, we will first check this for Insurance and Insurance information, and then for Insurance information and No insurance. Let's first focus on Insurance and Insurance information.

So we want to test if there is significant difference between the mean age of Insurance treatment and mean age of Insurance information treatment. Hence, our null hypothesis is that there is no difference between the mean age of treatment Insurance and Insurance information
$$H_0:\mu_{ins}-\mu_{ins.info}=0$$  

where $\mu_{ins}$ is the mean age of participants with treatment Insurance and $\mu_{ins.info}$ is the mean age of participants with treatment Insurance information.  

The next steps are to decide the level of significance, calculate the test statistic and p-values, and decide if we have sufficient evidence to reject the null hypothesis or not. Luckily, all this can be done in R with ease using a single command.

If you are new to hypothesis testing and t-tests, or would like to refresh your knowledge, then check the info boxes below.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Hypothesis Testing")
```


```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Welch's t-test")
```


In R, a two-sample Welch's t-test can be performed by using the function `t.test(x,y)`. A vector of *numeric* data values of the variable to be tested should be provided in the arguments x and y.  

Since we want to run a t-test to see if the average age of participants receiving treatment Insurance is significantly different from the average age of participants receiving treatment Insurance information, we will first filter the dataset for treatment Insurance and *pull* the age of these participants and store it in a vector. We will do the same for the treatment Insurance information. Then we will run a t-test on these two vectors.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("dplyr::pull")
```

**Exercise:** Go through the below code and run it.

```{r "4_2"}
# Extracting the ages of participants with treatment Insurance and storing it in vector x
x <- dat_2 %>% filter(treatment == "Insurance") %>% pull(age)
# Extracting the ages of participants with treatment Insurance information and storing it in vector y
y <- dat_2 %>% filter(treatment == "Insurance information") %>% pull(age)
# Running t-test to test for equality of mean ages of the treatments Insurance and Insurance information
t.test(x,y)
```

The value **t** shows the observed value of the test statistic, **df** denotes the degrees of freedom, and **p-value** indicates the p-value of the test statistic using which you can determine whether you can reject the null hypothesis or not. In a hypothesis test, the p-value is the probability of getting a value of the test statistic that is at least as extreme as the test statistic obtained from the sample data, assuming that the null hypothesis is true (Triola, Mario, 2019, p. 363). Put differently, it is the smallest significance level at which the null hypothesis would be rejected, given the observed test-statistic. If the p-value is **less** than the significance level $\alpha$, we have sufficient evidence to reject the null hypothesis at $\alpha$ level of significance. Otherwise, we do not have enough evidence to reject the null hypothesis.

Therefore, at a $5\%$ level of significance, we **cannot reject** the null hypothesis that the difference in the mean age of participants with treatment Insurance and participants with treatment Insurance information is equal to $0$, as the p-value $0.4456$ is greater than the level of significance $0.05$.

Equivalently, the $95\%$ confidence interval of the difference in means contains the hypothesized value $0$, hence we cannot reject the null hypothesis at $5\%$ level of significance.  


Now let's test if the average age of participants with treatment Insurance information is significantly different from the average age of participants with treatment No insurance.

**Exercise:** Replace the ___ with appropriate commands to extract the ages of participants with treatment Insurance information and No insurance and store it in vector x and y respectively. Then run a t-test to test if the age of the participants with treatment Insurance information is significantly different from the age of the participants with treatment No insurance.

```{r "4_3"}
# Replace ___ with appropriate dplyr commands to extract the ages of participants with treatment Insurance information and store it in vector x
x <- dat_2 %>% ___(treatment == "Insurance information") %>% ___(age)
# Replace ___ with appropriate dplyr commands to extract the ages of participants with treatment No insurance and store it in vector y
y <- dat_2 %>% ___(treatment == "No insurance") %>% ___(age)
# Run Welch's t-test to test for equality of mean ages of participants with treatment Insurance information and No insurance
___(x,y)
```

It's time for you to interpret the results!


Quiz: The null hypothesis of the above test is ...

[1]: There is difference in the mean age of participants with treatment Insurance information and mean age of participants with treatment No insurance.
[2]: There is no difference in the mean age of participants with treatment Insurance information and mean age of participants with treatment No insurance.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("t.test H0")
```

***


Quiz: Observe the p-value in the above results. Assume a 5% level of significance. Which of the following statements are correct?

[1]: The p-value is greater than the assumed significance level.
[2]: The p-value is smaller than the assumed significance level.
[3]: We have sufficient evidence to reject the null hypothesis at 5% level of significance.
[4]: We do not have sufficient evidence to reject the null hypothesis at 5% level of significance.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("t.test 1")
```

***


Quiz: Observe the confidence interval of the difference in means in the above result. Which of the following statements are correct?

[1]: The confidence interval does not contain the hypothesized value 0.
[2]: The confidence interval contains the hypothesized value 0.
[3]: We have evidence to reject the null hypothesis at 5% level of significance.
[4]: We do not have sufficient evidence to reject the null hypothesis at 5% level of significance.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("t.test 2")
```

***



Since we want to perform t-tests on multiple variables, I created a function `balance_test()` which will run the t-test on the variables specified in its input argument `var_list` and return a table with the necessary values as the output. Go through the code below if you are interested. Otherwise, just run it.
<!-- Check the info box below if you are interested in the code. -->

**Exercise:** Run the code below.

```{r "4_6"}
# Create a function for performing balance tests
balance_test <- function(data, var_to_filter, value_1, value_2, var_list){
  
  # Convert factors to numeric as we want numeric input in t.test()
  data <- data %>%
    mutate_if(is.factor,as.character) %>%
    mutate_at(all_of(var_list),as.numeric)
  
  # Converting the input argument var_to_filter to a symbol
  var_to_filter_sym <- rlang::sym(var_to_filter)
  
  # make a table for the balance test
  bal_test_full <- data.frame()
  
  # for loop to perform t.test() for all the variables in the list var_list
  for (i in 1:length(var_list)){
    
    # Converting the variable name to a symbol
    var_sym <- rlang::sym(var_list[i])
    
    # Extract the variable and store it in x and y
    x <- data %>% filter(!!var_to_filter_sym == value_1) %>% pull(!!var_sym)
    y <- data %>% filter(!!var_to_filter_sym == value_2) %>% pull(!!var_sym)
    
    # Run the t-test
    mean_t_test <- t.test(x,y)
    
    # storing the results in bal_test_df
    bal_test_df <- data.frame(Variable = var_list[i],
                              value_1 = round(mean_t_test$estimate[1],2),
                              value_2 = round(mean_t_test$estimate[2],2),
                              Difference = round(mean_t_test$estimate[1]-mean_t_test$estimate[2],2),
                              t = round(mean_t_test$statistic,4),
                              p_value = round(mean_t_test$p.value,2),
                              ci_l = round(mean_t_test$conf.int[1],4),
                              ci_u = round(mean_t_test$conf.int[2],4),
                              row.names = NULL)
    
    # joining the results of all the variables
    bal_test_full <- rbind(bal_test_full,bal_test_df)
    }
  
  # renaming some variables
  data.table::setnames(bal_test_full,
                       old = c('value_1','value_2'),
                       new = if (is.character(value_1) & is.character(value_2))
                         c(value_1,value_2)
                       else c(paste(var_to_filter,value_1,sep = "="),
                              paste(var_to_filter,value_2,sep = "=")))
  
  # return the balance test table
  return(bal_test_full)
  }
```

To use the function `balance_test(data, var_to_filter, value_1, value_2, var_list)` we need to specify the input parameters. The dataset can be specified in the argument `data`. In `var_to_filter` you have to specify the *variable* across which the comparison you want to make, and the *values* of this variable across which the comparison you want to make should be specified in `value_1` and `value_2`. You can provide the variable (or list of variables) for which you want to run the t-test in the input parameter `var_list`. For example, if you want to test the mean age of treatments Insurance and Insurance information, the command should be: `balance_test(data = dat_2, var_to_filter = "treatment", value_1 = "Insurance", value_2 = "Insurance information", var_list = "age")`.

We will now specify a list of variables for which we want to run the t-test, and store it in `variables`. Then, we will run the function `balance_test()` to check if these variables are significantly different for the treatment groups 'Insurance' and 'Insurance information' or not.

**Exercise:** Run the code below.

```{r "4_7"}
# Specify the variables you want to perform the balance test for
variables <- c("age","Female","tmonthlyincome","employed","higheducation",
               "ethnicity_akan","ethnicity_ewe","ethnicity_ga",
               "church_daily","pray_multiple","revival","Wave_2")

# Balance test for Insurance vs Insurance information
t1 <- balance_test(data = dat_2, var_to_filter = "treatment", value_1 = "Insurance",
                   value_2 = "Insurance information", var_list = variables)
t1
```

The column `Insurance` contains the mean of the corresponding variable for participants with treatment Insurance, and the column `Insurance information` contains the mean of the corresponding variable for participants with treatment Insurance information. `Difference` contains the difference between these two means. The column `t` shows the value of the t test statistic and the p-values are contained in column `p_value`. The columns `ci_l` and `ci_u` contain the lower and upper limit of the $95\%$ confidence interval of the difference in means.

The p-value for variable `revival` is $0.04$ which is less than the significance level $5\%$, alternatively observe that the $95\%$ confidence interval for variable revival does not contain the value $0$, therefore we can say that the participants with treatment Insurance information who attended the experiment during revival weeks are *significantly* more than participants with treatment Insurance who attended the experiment during revival weeks. The other variables are balanced for the treatments Insurance and Insurance information.  

Now we will run the `balance_test()` function to to check if the variables specified in `variables` are significantly different for the treatment groups 'Insurance information' and 'No insurance' or not.

**Exercise:** Replace the ___ with appropriate commands to run the function `balance_test()` on the data `dat_2` to test if the variables listed in the above list `variables` are significantly different for participants with treatment "Insurance information" and participants with treatment "No insurance".

```{r "4_8"}
# Balance test for Insurance information vs No insurance
t2 <- ___(data = dat_2, var_to_filter = "treatment", value_1 = ___,
                   value_2 = "No insurance", var_list = variables)
t2
```

Answer the quiz below.

***


Quiz: Which of the following statements are true?

[1]: The average monthly income of participants with treatment Insurance information is significantly different from those with treatment No insurance.
[2]: The participants with treatment Insurance information who attended the experiment during revival weeks are significantly more than participants with treatment No Insurance who attended the experiment during revival weeks.
[3]: The participants with ethnicity Ga Abende who received treatment Insurance information are significantly less than participants with ethnicity Ga Abende who received treatment No Insurance.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("balance_test 1")
```

***

Great work! Carry on to the next section to start with plotting!

$~$

## Exercise 3.3 Plotting variables across treatments

In this section we will plot gender, monthly income, and ethnicity, across the three treatment groups. 

First, we need to load the dataset again. 

**Exercise:** Replace the ___ with the appropriate command to load the dataset `dat_2.csv` and store it in `dat_2`.

```{r "5_1"}
# Load dat_2.csv and store it in dat_2
dat_2 <- ___("dat_2.csv")
```


Before we start with plotting, let's learn about another two important `dplyr` commands: `group_by()` and `summarise()`. The `group_by()` command takes an existing dataset and converts it into a *grouped* dataset which then allows operations to be performed within these groups. The `summarise()` command is used for generating desired summary statistic.

Let's look at an example to understand the working of these commands better.  

**Exercise:** Replace the ___ with appropriate command in order to get the mean monthly income of the dataset `dat_2`.

```{r "5_2"}
# Replace the ___ with appropriate commands to get the average monthly income
dat_2 %>%
  summarise(avg_monthly_income = ___(tmonthlyincome))
```

Hmm, strange, why do we get NA?

Well, we have some missing values for the variable `tmonthlyincome`. So, in order to remove the NA values before computing the mean, we need to specify `na.rm=TRUE` inside of the `mean()` function.

**Exercise:** Replace the ___ with appropriate commands in order to get the average monthly income of the dataset `dat_2`.

```{r "5_3"}
# Replace the ___ with appropriate commands to get the average monthly income
dat_2 %>% 
  summarise(avg_monthly_income = ___(tmonthlyincome, ___ = TRUE))
```

Good job! So the average monthly income of our dataset is $GHS~308.5179$. ($3.8~GHS$ was equal to $1~USD$ in 2015.)  

But what if we want to see the average monthly income for different treatments? This is one of the situations where `group_by()` comes in handy.  

**Exercise:** Run the code below.

```{r "5_4"}
# Average monthly income for different treatments
dat_2 %>% 
  group_by(treatment) %>% 
  summarise(avg_monthly_income = mean(tmonthlyincome, na.rm = T)) %>% 
  as.data.frame()
```

Here, the data `dat_2` is first grouped by the variable `treatment`, and the `summarise()` command that follows is then performed on this *grouped dataset*, thereby giving us the average monthly income for the different treatments.  

The participants of the treatment group 'Insurance' have an average monthly income of approx. $GHS~312$, ~ $GHS~311$ for the group 'Insurance information', and ~ $GHS~303$ for the 'No insurance' group.

Now we will learn how to visualize this information. A very powerful and commonly used package for graphics in R is the `ggplot2` package. This single package allows us to plot various types of plots like bar charts, line charts, box-plots, density plots, histograms etc. Generally, we start the code with `ggplot()`, inside which we can specify the dataframe and the aesthetics which will then be used for all the following layers. The dataframe can be specified using the `data` parameter, and the aesthetics, like the x and y variables, can be specified inside of `aes()`. We can add on layers like `geom_line()`,`geom_point()`, `geom_histogram()` etc. depending on the type of plot we desire, and we can specify the axes labels using `xlab()` and `ylab()`. Every layer is joined using `+`.

For more details you can check <https://cran.r-project.org/web/packages/ggplot2/ggplot2.pdf>.    

To understand it better, the first code has already been provided for you.

**Exercise:** Go through the below code and run it.

```{r "5_5",dev='svg'}
# Load the package ggplot2
library(ggplot2)

# Plotting a bar plot for average monthly income for different treatments
dat_2 %>% 
  group_by(treatment) %>% 
  summarise(average_income = mean(tmonthlyincome, na.rm = T)) %>% 
  ggplot(aes(x = treatment, y = average_income)) + # specifying the aesthetics
  geom_bar(stat = "identity") # adding layer geom_bar
```

On the x-axis we have the three treatment groups, and on the y-axis we have average monthly income in $GHS$. The height of the bars thus represent the average monthly income of the corresponding treatment group. So the plot tells us the same thing we found above, but this time in a visual way. 

Well, although the plot does convey the desired information, I have to admit, that's not the most appealing plot. But don't worry, we will make the next plots a little more elegant!  

Before we move on to the next plot, observe that we have used `stat = "identity"` inside of `geom_bar()`. But what does this mean? 

Every `geom` has a default `stat` for it. For `geom_bar()`, the default `stat` is `stat_count()`. What this means is, if we don't provide the y variable in `aes()` and use the default `stat` for `geom_bar()` (which is stat *count*), we will get the count of number of cases of the respective group on the y axis. However, if we want to provide our own y values (for example in our case we want to see the average income on the y axis), we can supply the y aesthetic in `aes()` and use the `stat` "identity".  


Moving on, now we want to plot the proportion of gender for the three treatment groups. But for that we first need to *calculate* the proportion of females and males within the three treatment groups. To do so, we will first group the data by `treatment` and `gender`, get the summary of the count of participants within each of these groups using `n()`, and then compute the gender proportions within the treatment groups.  

**Exercise:** Replace the ___ with appropriate dplyr commands to get the proportion of females and males for the three treatment groups.

```{r "5_6"}
# Replace the ___ with appropriate dplyr commands to get the gender proportion for the treatment groups
dat_2 %>% 
  ___(treatment, gender) %>% 
  ___(count = n()) %>% 
  mutate(proportion = count/sum(count))
```

The data is first grouped into the three treatment groups 'Insurance', 'Insurance information', and 'No insurance', and further sub-grouping of gender is done within each of the three groups. The `summarise()` and `mutate()` operations are then operated on this grouped data. 

We will now visualize the above information using similar `ggplot2` commands as before. But this time we will beautify the plots a little and also add confidence intervals using `geom_errorbar()`. The lower and upper values of the confidence interval can be provided using `ymin` and `ymax` aesthetics in the `aes()` argument.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Confidence Intervals")
```

**Exercise:** Replace the ___ with the appropriate ggplot2 commands to plot the proportion of gender for the different treatments.

```{r "5_7",dev='svg'}
# Replace ___ with appropriate ggplot2 commands to plot proportion of gender for the different treatment groups
dat_2 %>% 
  ___(treatment, gender) %>% 
  summarise(count = n()) %>% 
  mutate(proportion = count/sum(count),
         ci_l = proportion - qnorm(1-0.05/2)*sqrt(proportion*(1-proportion)/count),
         ci_u = proportion + qnorm(1-0.05/2)*sqrt(proportion*(1-proportion)/count)) %>% 
  ggplot(___(x = treatment, y = proportion, fill = gender)) +
  geom_bar(___ = "identity", 
           width = 0.9,
           position = "dodge", # position dodge places the bars side to side
           colour = "black") +
  geom_errorbar(aes(ymin = ci_l, ymax = ci_u),
                position = position_dodge(0.9),
                width = 0.3) +
  ggtitle("Gender Proportion Across Treatments") +
  ___("Treatments") +
  ylab("Proportion") +
  theme(panel.background = element_rect("ivory"),
        panel.grid = element_line("grey"))
```



Here we have treatment groups on the x-axis and proportion on the y-axis. The height of the bars represent the proportion of the gender *within* the respective group. The 95% CI shows that you can be 95% certain that the interval contains the true proportion of the respective gender of the population. Lower variability in data will result in a narrower confidence interval, whereas higher variability in data will result in a wider confidence interval.

Observe that we have the `fill` parameter *inside* of `aes()`. This allows us to fill the bars based on a variable. Further, observe the `position` argument in `geom_bar` ad `geom_errorbar`. Instead of a single vertical bar for each group, the position *dodge* allows us to plot the bars side by side.  


Now let's see how ethnicity is distributed across the treatments. This time we will make an interactive plot. To do so, we will use similar `ggplot2` commands to build the plot and store it in a variable. Then, we will call the command `ggplotly()` on this variable in order to transform our `ggplot2` object into a `plotly` object.

**Exercise:** Replace the ___ with the appropriate commands to plot the proportion of various ethnicities (variable `ethnicity`) for the different treatment groups.

```{r "5_8",dev='svg', output='htmlwidget', widget='plotly'}
# Load the package plotly
# Enter your code below:

# Replace ___ with appropriate commands to plot proportion of different ethnicities for the different treatment groups
p1 <- dat_2 %>%
  group_by(treatment, ___) %>%
  summarise(count = ___) %>%
  ___(proportion = count/sum(count)) %>%
  ggplot() +
  ___(aes(x = ___, y = proportion, fill = ethnicity),
           stat = ___,
           ___ = "dodge",
           colour = "black") +
  theme(panel.background = element_rect("ivory"),
        panel.grid = element_line("grey")) +
  ggtitle("Ethnicity Proportion Across Treatments") +
  xlab("Treatments") +
  ylab("Proportion")

ggplotly(p1)
```

You can hover over the bars to see the details.  

Carry on to the next section to plot the results of the dictator games!

$~$

## Exercise 3.4 Plotting results of dictator games

In this section we will plot the results of the three main dictator games *Self v.s. Church (anonymous)*, *Self v.s. Street Children (anonymous)*, and *Self v.s. Thanksgiving (anonymous)*. Before we begin, let us load the dataset again.  

**Exercise:** Load the dataset `dat_2.csv` and assign it to variable `dat_2`.

```{r "6_1"}
# Load the dataset dat_2.csv and assign it to variable dat_2
# Enter your code below:

```


'Revival weeks' are intense periods of church-going and fund-raising during which congregations typically give substantially more to church than normal (*paper*). In order to avoid the results being influenced by this, we will, at first, only consider the participants that attended the experiment during regular (non-revival) weeks.  

**Exercise:** Replace the ___ with the appropriate dplyr command to keep only the members who participated in the experiment during regular weeks.

```{r "6_2"}
# Replace the ____ with the appropriate dplyr command to keep only the members who participated in the experiment during regular weeks
df_norevival <- dat_2 %>% ___(revival == 0)
```


The variables `choice_keep_church`, `choice_keep_street`, and `choice_keep_thanks` contains the proportion of endowment given to participant's church, Street Children's Fund, and Thanksgiving Prayer Event, respectively. What do you think are the possible values that these variables can take? 

***


Quiz: Write down the minimum value that you think the variables `choice_keep_church`, `choice_keep_street`, and `choice_keep_thanks` can take.

Answer: 

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("min value quiz")
```

***


Quiz: Write down the maximum value that you think the variables `choice_keep_church`, `choice_keep_street`, and `choice_keep_thanks` can take.

Answer: 

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("max value quiz")
```

***

Well, since these variables are *proportions*, they can only take values between 0 and 1.   

Now, let's plot the results of the dictator game *Self v.s. Church (anonymous)* (variable `choice_keep_church`). As we are dealing with a continuous variable, we will plot a histogram for it. We will again use the `ggplot2` package for making the plots. In order to plot a histogram, we will use the geom `geom_histogram`.

**Exercise:** Go through the below code and run it by clicking on `edit` and then press `check`.
```{r "6_3",dev='svg'}
# Plot histogram for variable choice_keep_church
ggplot(df_norevival) +
  geom_histogram(aes(choice_keep_church),
                 fill = "turquoise", 
                 colour = "black") +
  xlab("Proportion of Endowment Given to Church") +
  ylab("Number of Participants") +
  theme(panel.background = element_rect("ivory"),
        panel.grid = element_line("grey"))
```

On the x-axis we have the proportion of endowment given to church, and on the y-axis we have the number of participants. The height of the bins indicate the number of participants the respective bin contains. We can observe that most of the participants are clustered at the center, followed by a gradual decrease on both sides, but there's a spike at the extreme ends 0 and 1. We will talk about this spike again in a moment. First, let's adjust the interval of the bins by choosing a different `binwidth`.

**Exercise:** Replace the ___ with the appropriate ggplot2 commands in order to plot a histogram for the variable `choice_keep_church`.
```{r "6_4",optional=TRUE, dev='svg'}
# Plot histogram for variable choice_keep_church
___(df_norevival) +
  ___(___(x = choice_keep_church),
                 binwidth = 0.08, # setting the width of the bins
                 fill = "turquoise",
                 colour = "black") +
  ___("Self v.s. Church") + # set the title of the plot
  ___("Proportion of Endowment Given to Church") + # set the x-axis label
  ___("Number of Participants") + # set the y-axis label
  theme(panel.background = element_rect("ivory"),
        panel.grid = element_line("grey"))

```

As the name suggests, `binwidth` controls the width of the bin. This value has to be in accordance with the continuous variable that you are plotting.

Now we will plot the histograms of all the three variables and view them together. Since the variables `choice_keep_church`, `choice_keep_street` and `choice_keep_thanks` represent the proportion of endowment given to church, Street Children's Fund, and Thanksgiving Prayer Event (vs. keeping with self) respectively, the remaining proportion, i.e. `1-choice_keep_church`, `1-choice_keep_street` and `1-choice_keep_thanks`, represent the proportion of endowment the participant kept with themselves. So, in order to get a common x-axis, we will plot this inverse of the variables. 
The function `grid.arrange` from the `gridExtra` package allows us to arrange multiple plots on a single page.

**Exercise:** Run the code below.

```{r "6_5",dev='svg'}
# Load the package gridExtra
library(gridExtra)

# Arrange the three plots using grid.arrange
grid.arrange(
  # results of Self vs. Church
  ggplot(df_norevival) +
    geom_histogram(aes(1-choice_keep_church), binwidth = 0.08,
                   fill="turquoise4", colour = "black") +
    ggtitle("Self v.s. Church") + xlab("") + ylab("") +
    theme(panel.background = element_rect("ivory"),
          panel.grid = element_line("grey")),


  # results of Self vs. Street Children's Fund
  ggplot(df_norevival) +
    geom_histogram(aes(1-choice_keep_street), binwidth = 0.08,
                   fill="turquoise4", colour = "black") +
    ggtitle("Self v.s. Street Children") + xlab("") + ylab("") +
    theme(panel.background = element_rect("ivory"),
          panel.grid = element_line("grey")),

  # results of Self vs. Thanksgiving prayer event
  ggplot(df_norevival) +
    geom_histogram(aes(1-choice_keep_thanks), binwidth = 0.08,
                   fill="turquoise4", colour = "black") +
    ggtitle("Self v.s. Thanksgiving") +
    xlab("Proportion of Endowment Kept with Self") + ylab("") +
    theme(panel.background = element_rect("ivory"),
          panel.grid = element_line("grey"))
  )
```
*This figure corresponds to Figure 1 of the paper.*


Observe that the result of all three dictator games have a roughly similar pattern: clustering in the middle, gradual decrease as we move along the tails, and a frequency spike at the extreme ends 0 and 1. This spike is probably seen because as we are dealing with proportions, it is not possible to go below 0 or above 1, *but*, hypothetically, *if* it was possible to keep less than nothing, or to keep more than the entire endowment, some of the participants clustered at these two extremes would have probably done so. However, as that is not a possibility, the participants are bound to, at most, either keep nothing, or keep everything, even if they *desire* to cross these limits. This observation is important when we are deciding the regression model to be chosen. We will come to this again in section 4.3.

I hope you are enjoying! Continue to the next section to learn about Regression modeling!

$~$


## Exercise 4.1 OLS Regression

In this section we will look at how the results of the three main dictator games are affected by enrollment into insurance, in contrast to just receiving information about insurance, i.e. we will see how the treatment *Insurance* impacts giving to church and other recipients as compared to *Insurance information*.

Before we start we need to load the dataset again.

**Exercise:** Use the `read.csv()` command to load the dataset `df_norevival.csv` and store it in the variable `df_norevival`.

```{r "7_1"}
# Use read.csv() to load df_norevival.csv and store it in df_norevival
# Enter your code below:

```


As we only want to compare the treatment group *Insurance* to the control group *Insurance information*, let us remove the members of the *No insurance* treatment from this dataset.

**Exercise:** Replace the ___ with appropriate commands to remove members of the treatment group 'No insurance' from the dataset `df_norevival`. Store the results in `df_norevival_1`.

```{r "7_2"}
# Replace ___ appropriately in order to remove treatment group 'No insurance' from the data df_norevival, and store the results in 'df_norevival_1'
___ <- df_norevival %>% ___(treatment != ___)
```

This dataset `df_norevival_1` contains all the participants of treatment group *Insurance Enrollment* or control group *Insurance Information* who took part in the experiment during regular (non-revival) weeks.  

Let's start with the results of the dictator game 'Self vs. Church (anonymous)'. Recall that the variable `choice_keep_church` contains the proportion of endowment given anonymously to the church, and the remaining proportion kept with self, and the variable `Tinsurance` has value '1' if the participant was a part of the treatment group *Insurance*, and '0' otherwise. Since we have removed the participants of treatment group *No insurance*, here the '0' in `Tinsurance` would thus indicate participants with treatment *Insurance information*.  
We are interested in observing how enrollment into insurance affects the proportion of endowment given anonymously to the church (vs. kept with self). Intuitively, one way to solve this is to think of this as a regression problem. In the quiz below take a guess at which variable you think is the dependent and independent variable.

***


Quiz: In a regression problem where we want to observe how enrollment into insurance affects the proportion of endowment given to the church vs. kept with self, which do you think is the dependent and independent variable?

[1]: Tinsurance is the independent variable and choice_keep_church is the dependent variable.
[2]: choice_keep_church is the independent variable and Tinsurance is the dependent variable.
[3]: It does not matter which one we take as dependent or independent variable.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("dep ind var quiz")
```

***

Since we want to see how the proportion of endowment given to church is *dependent* on the treatment Insurance Enrolment, `choice_keep_church` is our dependent variable (also known as response variable), and `Tinsurance` is our independent variable (also known as explanatory variable). The regression equation is:

$$choice\_keep\_church_i = \beta_0 + \beta_1 ~  Tinsurance_i + \epsilon_i~,~~i=1,...,538$$

where $\beta_0$ and $\beta_1$ are the unknown parameters to be estimated, and $\epsilon$ is the error term. The subscript $i$ denotes the $i^{th}$ observation. (There are 538 observations in `df_norevival_1`.)


```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Ordinary Least Squares Estimation")
```


```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Assumptions of the Linear Regression Model")
```

What do you think the parameters represent? Answer the quizzes below.


***


Quiz: In the equation $\hat{choice\_keep\_church} = \hat\beta_0 + \hat\beta_1 ~  Tinsurance$, what can you say about the coefficient $\hat{\beta_1}$? You can select multiple options.

[1]: It can only take positive values.
[2]: It shows the expected change in response variable choice_keep_church due to one unit increase in the explanatory variable Tinsurance.
[3]: It can never be 0.
[4]: None of the above.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("beta 1 quiz")
```

***


Quiz: The coefficient $\hat{\beta_0}$ in the equation $\hat{choice\_keep\_church} = \hat\beta_0 + \hat\beta_1 ~  Tinsurance$ indicates the expected proportion of endowment given to church when $Tinsurance$ is 0.

[1]: True.
[2]: False.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("beta 0 quiz")
```

***

In R, we can fit a **l**inear **m**odel using the `lm()` command. The parameter `formula` contains the regression equation in the form *dep.var ~ ind.var1 + ind.var2 + ...* and the parameter `data` contains the dataset to be used. The first model where we are estimating the effect of treatment Insurance on the proportion of endowment given to Church has been done for you.

**Exercise:** Go through the below code and run it.

```{r "7_3"}
# Fit a linear model on data 'df_norevival_1' with 'choice_keep_church' as the dependent variable and 'Tinsurance' as the independent variable to estimate the effect of treatment Insurance on proportion of endowment given to church
ols_church <- lm(choice_keep_church ~ Tinsurance, data = df_norevival_1)
```

Now let's take a look at the summary of the results.

**Exercise:** Call the command `summary()` on `ols_church`.

```{r "7_4"}
#  Call summary() on ols_church
# Enter you code below:
```


Let's interpret the results.

The first section, **Residuals**, give us information about the symmetry of the distribution of residuals. One of the assumptions of using linear models, albeit generally an unimportant one, is that the errors should follow a Normal distribution. Hence, the residuals, which are estimates of the error terms, should follow a Normal distribution too. To check this, we can check if the median of the residuals is close to $0$ (since the mean of the residuals is $0$, and for a normal distribution mean = median = mode). Further, the first and third quartile, and the min and max value should be similar in magnitude respectively (they would be equal in magnitude for a symmetric distribution with mean $0$).  


The next section, **Coefficients**, contains information about the estimated coefficients, their standard errors, t statistics, and p-values:

- **Estimate:**
The column *Estimate* contains the estimated coefficients.

    + The **intercept** indicates the expected response when all the explanatory variables are $0$. Here, the value $0.54830$ means the average proportion of endowment given to church when the participants are from the control group Insurance Information (i.e. when our explanatory variable `Tinsurance` is '0') is $0.54830$.

    + The rows below intercept contain estimates of coefficients of independent variables and indicate the expected change in the response variable due to one unit change in the explanatory variable. Since we only have one explanatory variable in our model - **Tinsurance**, and as it is a binary variable with value '0' or '1', its estimated coefficient of $-0.05907$ indicates that the treatment Insurance Enrollment (i.e. when `Tinsurance` is '1') ***decreases*** the proportion of endowment given to church *by $5.9\%$*.


- **Std. Error:**
This column shows the standard error of the estimated coefficients. Standard errors show how far, on average, the estimates are from the actual values. Hence, smaller the standard errors relative to the estimators, better is the estimation. 


- **t value:**
The column *t value* contains the observed test-statistic. It denotes the number of standard errors the coefficient is away from zero. For large t-values the null hypothesis that the true coefficient is $0$ is rejected.


- **Pr(>|t|):**
This column shows the p-value. In a hypothesis test, the p-value is the probability of getting a value of the test statistic that is at least as extreme as the test statistic obtained from the sample data, assuming that the null hypothesis is true (Triola, Mario, 2019, p. 363). Put differently, it is the smallest significance level at which the null hypothesis would be rejected, given the observed t-statistic. Usually, a p-value of $5\%$ or lower is a good indicator for the significance of a variable. We can observe that the p-value of Tinsurance ismuch lower than $0.05$.


- **Significance level:**
The significance level denotes the probability of rejecting the null hypothesis when it is true. This is also known as Type I error. The stars denote the significance level. As you can see in *Signif. Codes*, one star denotes p-value of less than $0.05$, two stars denote p-value of less than $0.01$ and three stars denote a p-value of less than $0.001$. The coefficient of `Tinsurance` is significant not just at $5\%$ level of significance, but also at $1\%$ level of significance.


***

We will now fit a linear model on the dataset `df_norevival_1` to estimate the effect of treatment Insurance on the proportion of endowment given to Thanksgiving prayer event. 

**Exercise:** Fit a linear model using the `lm()` command on the dataset `df_norevival_1` to estimate the effect of treatment Insurance (`Tinsurance`) on the proportion of endowment given to Thanksgiving prayer event (`choice_keep_thanks`). Store the results in `ols_thanks`.

```{r "7_5"}
# Fit a linear model on data 'df_norevival_1' to estimate the effect of treatment Insurance (variable `Tinsurance`) on the proportion of endowment given to Thanksgiving prayer event (variable `choice_keep_thanks`). Store the results in 'ols_thanks'.
# Enter your code below:
```


Before we look at the results, we will first run another model to estimate the effect of treatment Insurance on the proportion of endowment given to Street Children's Fund.  

**Exercise:** Fit a linear model on the dataset `df_norevival_1` to estimate the effect of treatment Insurance (`Tinsurance`) on the proportion of endowment given to Street Children's Fund (`choice_keep_street`). Store the results in `ols_street`.

```{r "7_6"}
# Fit a linear model on the dataset `df_norevival_1` to estimate the effect of treatment Insurance (variable `Tinsurance`) on the proportion of endowment given to Street Children's Fund (variable `choice_keep_street`). Store the results in `ols_street`
# Enter your code below:
```

Now let's look at the results. But this time, instead of using `summary()`, we will use the command `stargazer()` from the `stargazer` package. The `stargazer()` function makes an elegant summary table for the regression results and it also allows us to compare and interpret more than one regression result at the same time. So let's have a look at the results of the above three models together.

**Exercise:** Run the code below.

```{r "7_7",results='asis'}
# Load the library stargazer
library(stargazer)

# Combining the ols results ols_church, ols_thanks, and ols_street using stargazer()
stargazer(ols_church, ols_thanks, ols_street, type = "html")
```

$~$

In the above results, you can see the coefficients of the parameters along with their standard errors in brackets. The stars next to the values indicate the significance level. Three stars mean a p-value less than $0.01$, two stars mean a p-value less than $0.05$ and one star means a p-value of less than $0.1$. We can see that 'Tinsurance' has a *negative* coefficient in all three cases, and all three effects are significant at at least 5% level. Therefore, according to the above results, enrollment in Insurance *reduces* giving to church and other recipients.
  

$~$

Carry on to the next section to learn about controlling for variables in regression analysis!

$~$


## Exercise 4.2 OLS with Control Variables

In the previous section we found that the treatment Insurance enrolment reduces the proportion of endowment given to Church, Thanksgiving  Prayer Event, and Street Children's Fund. But how do we know that this is actually due to the difference in treatment, and not due to some other factor like gender or monthly income or any other variable? 

You might have heard the phrase *Correlation does not imply causation*. That is, just because we see two variables are correlated, it does not mean one causes the other. The relationship that we see could also be due to random chance or caused by some other variable that gets hidden in the error term. If the explanatory variable is correlated with the error term, the explanatory variable is said to be **endogenous**, and this is known as **endogeneity problem**. Then how do we know if the relationship that we see is causal?

Well, we can't exactly *prove* that the relationship is causal, but there are ways to see if it is more or less likely.

One effective way is to conduct a **randomized experiment**. Randomization in an experiment means that the treatments were assigned randomly to the participants. A good randomization minimizes the possibility of bias and allows for reliable estimation of treatment effects. In this paper, the authors had conducted a randomized experiment, and as we have seen in section 3.2, almost all the variables were balanced across the treatments. 

Another common way is by adding **control variables** to the model. Controlling for variables is done by holding them constant in the experiment. Control variables help in isolating the treatment effect and ensuring its causality on the outcome.  

Even if a randomized experiment has been conducted, researchers often still add control variables in order to isolate the causal effect of treatment. So although we have previously seen that the different variables were more or less balanced across the different treatments, we will nonetheless control for these variables to isolate the treatment effect.  

Before we begin we need to load the dataset again.

**Exercise:** Load the dataset `data_norevival_1.csv` and store it in `data_1`

```{r "8_1"}
# Load data_norevival_1.csv and store it in data_1

```

We will be controlling for the socio-demographics of participants (gender, age, ethnicity, high school education, employment status, monthly income), level of religiosity (daily visit to church, praying multiple times a day), the church of the participant, and the wave in which the participant took part in. You can check the info box below if you want to recall what these variables contain.   


```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("Variable definitions")
```

Before we fit the model, we need to convert our categorical variables to dummy variables. Dummy variables are dichotomous variables with value 1 if the attribute is present, and 0 otherwise. Using dummy variables is helpful as they allow us to use a single regression equation to represent multiple groups. For a categorical variable with $n$ levels, we need to create $n-1$ dummy variables lest you fall into the dummy variable trap! 

***


Quiz: Suppose we have a variable "Season" with 4 possible values - winter, spring, summer, and autumn. We created dummy variables called Winter, Spring, and Summer, which take the value 1 when that is the season. But how do we know when it is autumn?

[1]: When the dummy variables winter, spring, and summer take the value 0, it represents autumn.
[2]: We can't know for sure.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("dummy variable quiz")
```

***

In the dataset we already have 10 dummy variables for the variable `church_name_grouped` which represents different churches, and 3 dummy variables for variable `ethnicity`. We will now create a dummy variable for female gender, and a dummy variable for the second wave. Since in our dataset both these variables only have 2 levels we need to create 1 dummy for each of them.

**Exercise:** Replace ___ with the appropriate `dplyr` command to create variables `gender_fem_dummy` and `wave_2_dummy` in the dataset `data_1` and store it in `data_2`.

```{r "8_2"}
# Creating a dummy variable for gender and wave
data_2 <- data_1 %>% 
  ___(gender_fem_dummy = ifelse(gender == "Male", 0, 1),
      wave_2_dummy = ifelse(wave == 1, 0, 1))
```


Let us now run a linear regression model with independent variable `Tinsurance` and control variables `age`, `gender_fem_dummy`, `high_education`, `employed`, `log_tmonthlyincome`, `church_daily`, `pray_multiple`, `ethnicity_akan`, `ethnicity_ewe`, `ethnicity_ga`, `Dchurch1`, `Dchurch2`, `Dchurch3`, `Dchurch4`, `Dchurch5`, `Dchurch6`, `Dchurch7`, `Dchurch8`, `Dchurch9`, `Dchurch10`, and `wave_2_dummy`, to estimate the effect of Insurance treatment on the proportion of endowment given to participant's church. The control variables can be added to the model in the same way as we add additional explanatory variables: by using `+` as the separator.    

**Exercise:** Run the code below.

```{r "8_3"}
# OLS model with control variables to estimate the effect of treatment Insurance 
# on proportion of endowment given to church
ols_church_cv <- lm(choice_keep_church ~ Tinsurance + age + gender_fem_dummy + 
                      higheducation + employed + log_tmonthlyincome + 
                      church_daily + pray_multiple + ethnicity_akan + 
                      ethnicity_ewe + ethnicity_ga + Dchurch1 + Dchurch2 + 
                      Dchurch3 + Dchurch4 + Dchurch5 + Dchurch6 + Dchurch7 +
                      Dchurch8 + Dchurch9 + Dchurch10 + wave_2_dummy,
                    data = data_2)
```

We will see the results shortly. Let's run another OLS model with the same control variables to estimate the effect of Insurance treatment on the proportion of endowment given to Street Children's Fund. But that's a lot of control variables to type each time.. Well, instead of typing all the variable names every time, we can also store the names in a character vector, and then call this vector whenever we want to use those variables. So, let's store the response variable in `dep_var`, independent variable in `ind_var`, and the control variables in `control_vars`. Then we will use the commands `paste()` and `as.formula()` to get it as a formula. 

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("paste()")
```


**Exercise:** Go through the below code and run it.

```{r "8_4"}
dep_var <- "choice_keep_street"

ind_var <- "Tinsurance"

control_vars <- c("age", "gender_fem_dummy", "higheducation", "employed", "log_tmonthlyincome",
                  "church_daily", "pray_multiple", "ethnicity_akan", "ethnicity_ewe",
                  "ethnicity_ga", "Dchurch1", "Dchurch2", "Dchurch3", "Dchurch4", "Dchurch5",
                  "Dchurch6","Dchurch7", "Dchurch8", "Dchurch9", "Dchurch10", "wave_2_dummy")

ols_street_cv <- lm(as.formula(paste(dep_var, "~", ind_var, "+", paste(control_vars, collapse = "+"))),
                    data = data_2)
```

Let's run another model before looking at the results. This time run an OLS model on the dataset `data_2` with the same control variables to estimate the effect of Insurance treatment on the proportion of endowment given to Thanksgiving Prayer Event.  

**Exercise:** Run a linear regression model on the data `data_2` to estimate the effect of Insurance treatment on the proportion of endowment given to Thanksgiving Prayer event. To do so, first store the dependent variable as a string in variable `dep_var`. Then replace the ___ with the appropriate commands to run the OLS model with the same control variables as before.

```{r "8_5"}
# Store the correct dependent variable in dep_var to estimate the effect of Insurance treatment on the proportion of endowment given to Thanksgiving Prayer event
# Enter the code below:

# Replace the ___ with appropriate commands to run a linear model with control variables to estimate the effect of treatment Insurance on proportion of endowment given to Thanksgiving prayer event
ols_thanks_cv <- ___(as.formula(___(dep_var, "~", ind_var, "+", paste(control_vars, collapse = "+"))),
                    data = ___)
```

Let us now see the results of the above three OLS models with control variables along with the OLS results without control variables from section 4.1. We will again use `stargazer()` function from the `stargazer` package for this. As we are mainly interested in the effect of the treatment on the response variable, we will exclude the coefficients of control variables from the stargazer results by using the argument `omit=control_vars`.  


**Exercise:** Run the code below.

```{r "8_6",results='asis'}

# OLS models without control variables
ols_church <- lm(choice_keep_church ~ Tinsurance, data = data_2)
ols_street <- lm(choice_keep_street ~ Tinsurance, data = data_2)
ols_thanks <- lm(choice_keep_thanks ~ Tinsurance, data = data_2)

# Comparing the ols results (with and witout control variables) using stargazer()
stargazer(ols_church, ols_church_cv, ols_street, ols_street_cv, ols_thanks, ols_thanks_cv, 
          omit = control_vars, type = "html",
          dep.var.labels.include = FALSE,
          column.labels = c("Giving to church","Giving to street","Giving to thanksgiving"),
          column.separate = c(2,2,2), omit.stat=c("f", "ser"))
```

The odd numbers (1), (3), (5) correspond to the OLS results without control variables, and the even numbers (2), (4), (6) correspond to the OLS results with control variables. We can see that 'Tinsurance' has a negative coefficient in all the cases, and all the effects are significant. This means that enrollment in Insurance *reduces* giving to church and other recipients. Further, compared to the OLS models without control variables, the OLS models with control variables have a higher R-squared. The R-squared shows the goodness-of-fit of the model - it indicates how much variability of the response variable is explained by the independent variables.  

In the next section we will look at another regression model for estimating the treatment effects - the Tobit model.


$~$

## Exercise 4.3 Tobit Regression

In the previous sections we looked at linear regression using OLS to understand how the treatment Insurance Enrollment affects the proportion of endowment given to church. Now we will try to understand why OLS might not be the best model in our case. Let us first load the dataset again.

**Exercise:** Load the dataset `data_2.csv` and store it in `data_2`.

```{r "9_1"}
# Load data_2.csv and store it in data_2

```


Recall that in section 3.3 we had plotted the frequency plot for the variables `choice_keep_church`, `choice_keep_street`, and `choice_keep_thanks`. Let us have a look at it again.

```{r eval=FALSE}
# Run for additional info in the Viewer pane
info("preknit_JGwxvwTDMWnC")
```

Observe that for all three variables there is a frequency spike at 0 and 1, i.e. there is a clustering of the participants at the extreme ends 0 (keeping the entire endowment for themselves) and 1 (giving the entire endowment away). This indicates that had it been possible to give less than 0 (i.e. *take* money from the recipient instead of giving it), or give more than the entire endowment, some of the participants clustered at the two bounds 0 and 1 would have probably done so. However, since this is not possible as proportion cannot be below 0 or greater than 1, we can say that the *real value* of the proportion of endowment given to church is *not observable* for these cases and the data is *censored* at 0 and 1 (*(left-)censored from below* at 0 and *(right-)censored from above* at 1).
So a censored variable is one where the *real* value of the response variable is *not observable* for all the observations because the values reported can only be within a restricted range.

One possible way to deal with this kind of censored data is to use a Tobit regression model:


### Tobit Regression

Response variable $y$ is said to be censored when we can observe explanatory variable $x$ for all observations, but we know the *true* value of $y$ only for a limited range of observations; the values of $y$ for a certain range are reported as a single value or there is significant clustering around a value.
* If $y_i = y_L$ or $y_i > y_L$ for all observations $i = 1,2,...,n$, (and there is clustering at $y_L$) then $y$ is left-censored or censored from below at $y_L$.
* If $y_i = y_U$ or $y_i < y_U$ for all observations $i = 1,2,...,n$, (and there is clustering at $y_U$) then $y$ is right-censored or censored from above at $y_U$.
* If $y_i = y_L$ or $y_i = y_U$ or $y_L < y_i < y_u$ for all observations $i = 1,2,...,n$, (and there is clustering at $y_L$ and $y_U$) then $y$ is censored from both sides - censored from below at $y_L$ and censored from above at $y_I$.

We usually think of an uncensored $y$, $y^*$, as the *true value* of $y$ when the censoring mechanism is not applied. Typically, all the observations for $y$ are observable, but for $y^*$ are not observable. We call $y^*$ the *latent variable*. Latent variables are variables that are not directly observed, but are inferred from other variables that are observed.  

So in a Tobit regression model (a.k.a *censored regression model*), the response variable $y_i$ is expressed in terms of an underlying *latent variable* $y_i^*$, and this latent variable is a linear function of explanatory variables $x$, coefficients $\beta$, and normally distributed error term $\epsilon$ with mean $0$ and variance $\sigma^2$:
$$y_i^* = {\sf{{x_i}'}}\beta + \epsilon_i,~ \epsilon|{\sf{x}} \sim N(0,\sigma^2)$$

$$y_i = \begin{cases}y_i^*~~~if ~ y_L \lt y_i^* \lt y_U \\\\ y_L~~~if~y_i^* \le y_L \\\\ y_U~~~if~y_i^* \ge y_U\end{cases}$$
Here, $y_L$ represents the left-censoring point, and $y_U$ represents the right-censoring point.

The coefficients can be estimated using Maximum Likelihood Estimation (MLE). For more details you can read Tobin, James (1958, pp. 24–36).  


It is important to note that the coefficient $\beta$ is the effect of $x_i$ on the *latent* variable $y_i^*$ (and not the observed variable $y_i$ as in the case of linear regression model). So when we are interested in the effect of explanatory variable $x$ on latent variable $y^*$, i.e. $E[y^*|{\sf{x}}]$, then estimating the coefficient of $\beta$ is enough. However, when we are interested in the effect of explanatory variable $x$ on the observed variable $y$, i.e. $E[y|{\sf{x}}]$, then we need to apply some transformations in order to get the marginal effect of $x$ on $y$. 

In our case, $y^*$ cannot be observed due to measuring constraints (as we are dealing with proportions), but we are actually interested in the effect of treatment on this unobserved latent variable $y^*$, i.e. we are interested in $E[y^*|{\sf{x}}]$, and hence **do not** need to perform any transformations on the Tobit regression results.  

For more details on Tobit regression you can check these [lecture notes][bauer_link].
[bauer_link]: https://www.bauer.uh.edu/rsusmel/phd/ec1-23.pdf

***


In R, the `AER` package allows for Tobit regression modeling using the command `tobit(formula, left, right, data)`. We can specify the lower and upper limits in the parameters `left` and `right` respectively.

We will again start with estimating how the treatment Insurance Enrollment affects the proportion of endowment given to the church. This time we will fit a Tobit model with left censoring at 0 and right censoring at 1, and with control variables as discussed in section 4.2. The first one has been done for you.

**Exercise:** Go through the below code and run it.

```{r "9_3"}
# Load the package AER
library(AER)

# dependent variable
dep_var <- "choice_keep_church"
  
# independent variable 
ind_var <- "Tinsurance"

# control variables
control_vars <- c("age", "gender_fem_dummy", "higheducation", "employed", "log_tmonthlyincome",
                  "church_daily", "pray_multiple", "ethnicity_akan", "ethnicity_ewe",
                  "ethnicity_ga", "Dchurch1", "Dchurch2", "Dchurch3", "Dchurch4", "Dchurch5",
                  "Dchurch6","Dchurch7", "Dchurch8", "Dchurch9", "Dchurch10", "wave_2_dummy")

# Tobit regression with control variables
tobit_church_cv <- tobit(as.formula(paste(dep_var, "~", ind_var, "+", paste(control_vars, collapse = "+"))),
                      left = 0,
                      right = 1,
                      data = data_2)

# Call the summary() function on tobit_1
summary(tobit_church_cv)
```

Notice that 34 observations were not deleted before running the model due to missing information. Out of the remaining 504, 22 were left-censored i.e. 22 observations had gave nothing to the church (`choice_keep_church` = 0), and 38 were right-censored i.e. 38 gave everything to the church (`choice_keep_church` = 1). Since we are interested in the effect of insurance on the proportion of endowment given to church, we will focus only on the coefficient of Tinsurance which is -0.0557. The negative sign indicates that enrollment into insurance **reduces** giving to church, and the value indicates by how much. So according to the Tobit model, enrollment into insurance, as compared to only receiving information about insurance, reduces giving to church by 5.6%. Looking at the p-value we can see that this is significant at 5% level.

Now let's run a Tobit model to estimate the effect of Insurance treatment on the proportion of endowment given to Thanksgiving Prayer Event. Since the independent variable and control variables are the same as before, and just the dependent variable is changing from `choice_keep_church` to `choice_keep_thanks`, we only need to reassign the `dep_var` variable.  


**Exercise:** Run a Tobit model on the data `data_2` to estimate the effect of Insurance treatment on the proportion of endowment given to Street Children's Fund. To do so, first store the dependent variable as a string in variable `dep_var`. Then replace the ___ with the appropriate commands to run the Tobit model.

```{r "9_4"}
# Store the correct dependent variable as a string in `dep_var`

# Replace ___ with approporiate commands to run a Tobit regression model with control variables on the dataset data_2 to estimate the effect of Insurance treatment on proportion of endowment given to Street Children's Fund. Use the appropriate left and right censoring.
tobit_street_cv <- ___(as.formula(paste(dep_var, "~", ind_var, "+", paste(control_vars, collapse = "+"))),
                         left = ___,
                         right = ___,
                         data = ___)
```

Great! We will see the results of this shortly. First, run another tobit model on the dataset `data_2` to estimate the effect of Insurance treatment on the proportion of endowment given to Thanksgiving Prayer Event.  

**Exercise:** Run a Tobit model on the data `data_2` to estimate the effect of Insurance treatment on the proportion of endowment given to Thanksgiving Prayer event. To do so, first store the dependent variable as a string in variable `dep_var`. Then replace the ___ with the appropriate commands to run the Tobit model.

```{r "9_5"}
# Store the correct dependent variable as a string in `dep_var`

# Replace ___ with approporiate commands to run a Tobit regression model with control variables on the dataset data_2 to estimate the effect of Insurance treatment on proportion of endowment given to Thanksgiving prayer event. Use the appropriate left and right censoring.
tobit_thanks_cv <- ___(___(paste(dep_var, "~", ind_var, "+", paste(control_vars, collapse = "+"))),
                         left = ___,
                         right = ___,
                         data = ___)
```


Now let's have a look at the results of the above three Tobit models along with the OLS control variables models from the previous section 4.2. We will again use `stargazer()` function from the `stargazer` package for this. As we are mainly interested in the effect of the treatment on the response variable, we will again exclude the coefficients of control variables from the stargazer results by using the argument `omit=control_vars`.

**Exercise:** Run the code below.

```{r "9_6",results='asis'}
# OLS results with control variables 
ind_var <- "Tinsurance"
control_vars <- c("age", "gender_fem_dummy", "higheducation", "employed", "log_tmonthlyincome",
                  "church_daily", "pray_multiple", "ethnicity_akan", "ethnicity_ewe",
                  "ethnicity_ga", "Dchurch1", "Dchurch2", "Dchurch3", "Dchurch4", "Dchurch5",
                  "Dchurch6","Dchurch7", "Dchurch8", "Dchurch9", "Dchurch10", "wave_2_dummy")
# ols church cv
ols_church_cv <- lm(as.formula(paste("choice_keep_church", "~", ind_var, "+", paste(control_vars, collapse = "+"))),
                    data = data_2)
# ols street cv
ols_street_cv <- lm(as.formula(paste("choice_keep_street", "~", ind_var, "+", paste(control_vars, collapse = "+"))),
                    data = data_2)
# ols thanks cv
ols_thanks_cv <- lm(as.formula(paste(choice_keep_thanks, "~", ind_var, "+", paste(control_vars, collapse = "+"))),
                    data = data_2)


# Combining the ols and tobit results using stargazer()
stargazer(ols_church_cv, tobit_church_cv, ols_street_cv, 
          tobit_street_cv, ols_thanks_cv, tobit_thanks_cv, 
          omit = control_vars, type = "html",
          model.names = FALSE, dep.var.labels.include = FALSE,
          column.labels = c("Giving to church","Giving to street","Giving to thanksgiving"),
          column.separate = c(2,2,2), omit.stat=c("f", "ser","wald"))
```

$~$

The odd numbers (1), (3), and (5) correspond to the OLS models and the even numbers (2), (4), and (6) correspond to the Tobit models. Answer the below quiz.

***


Quiz: Which of the following is correct?

[1]: According to the Tobit model, enrollment into insurance (as compared to only information about insurance) increases giving to street children's fund by 6.9%.
[2]: According to the Tobit model, enrollment into insurance (as compared to only information about insurance) reduces giving to street children's fund by 6.9%.
[3]: Enrollment into Insurance decreases giving to church, street children's fund and thanksgiving event.
[4]: Enrollment into insurance increases giving to church, street children's fund and thanksgiving event.

```{r eval=FALSE}
# Run line to answer the quiz above
answer.quiz("tobit results")
```

***

(questions - preknit before stargazer results, embedding google form, plotly plot not interactive, skimr no space in output)


$~$


## Exercise 5 Conclusion

Congratulations! You are now at the end of the problem set. Let's recap what you've done so far and the conclusions that we can draw from it.

We began the problem set with an introduction to the problem - to see whether religious institutions play a role in providing informal insurance.
We started by exploring the data and looking at its contents, followed by balance tests to be sure of the randomness of the randomized experiment. Then we performed a graphical analysis of the data where we learned how to make plots.  

We then started with regression analysis to see if and how enrollment into insurance, as compared to only receiving information about insurance, affects the proportion of giving to church and other recipients. To do so, we used the OLS method, the OLS method with control variables, and Tobit regression with control variables. All three models suggested that enrollment into insurance **reduces** giving to church and other recipients. This indicates that the people might view church also as a source of insurance - as when they were insured, their giving to church reduced.  

Further, the authors had performed a Tobit regression to check how only receiving information about insurance, as compared to neither receiving information nor getting enrolled in insurance, affects the proportion of giving to church and other recipients. The regression results suggested that giving to church and other recipients is **increased** in this case. This further ensures that in our setting the Pentecostal church plays an important role in providing informal insurance, as reminding the participants about the uncertainties of life and providing them information about insurance (but not enrolling them in it) increased their givings to church.  


Pentecostal churches stress the involvement of God in terms of blessings in everyday life and teach about God rewarding religious and charitable giving. This particular religious discourse makes members of these churches more prone to see charitable behavior as a means to decrease the risk of bad events happening and to increase the occurrence of good events (*paper*).
Hence looking at the experiment results we can conclude that in such religious scenarios combined with a setting where formal insurance is not well established, religious institutions can play an important role and should help in spreading formal insurance rather than being an obstacle to its development.

***


## Exercise 6 References

### Bibliography

* Cartwright, E. (2018): *Behavioral Economics*. 3rd Edition, Routledge.  

* Emmanuelle Auriol, Julie Lassébie, Amma Panin, Eva Raiber, Paul Seabright: *God Insures those Who Pay? Formal Insurance and Religious Offerings in Ghana*, The Quarterly Journal of Economics, Volume 135, Issue 4, November 2020, Pages 1799–1848, https://doi.org/10.1093/qje/qjaa015  

* Engel, Christoph (2010): *Dictator Games: A Meta Study* https://homepage.coll.mpg.de/pdf_dat/2010_07online.pdf  

* Gujarati, D. N., & Porter, D. C. (2009): *Basic econometrics*. 5th Edition, McGraw-Hill.  

* Taeger, D., & Kuhnt, S. (2014): *Statistical hypothesis testing with SAS and R*, (pp.28-31), John Wiley & Sons.  

* Tobin, J. (1958): *Estimation of relationships for limited dependent variables*. Econometrica, 26(1), 24-36. https://doi.org/10.2307/1907382  

* Triola, M. F. (2018). *Essentials of statistics*. 6th Edition, Pearson.  

* William H. Greene (2018): *Econometric Analysis*. 8th Edition, Pearson.  

* Wooldridge J.M. (2016): *Introductory econometrics: A modern approach*. 6th Edition, Cengage Learning.  

* Bauer College of Business (Lecture Notes): https://www.bauer.uh.edu/rsusmel/phd/ec1-23.pdf  


### Packages in R

* Baptiste Auguie (2017). *gridExtra: Miscellaneous Functions for "Grid" Graphics*. R package version 2.3. https://CRAN.R-project.org/package=gridExtra
  
* C. Sievert. *Interactive Web-Based Data Visualization with R, plotly, and shiny*. Chapman and Hall/CRC Florida, 2020.

* Christian Kleiber and Achim Zeileis (2008). *Applied Econometrics with R*. New York: Springer-Verlag. ISBN 978-0-387-77316-2. URL https://CRAN.R-project.org/package=AER
  
* Elin Waring, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu and Shannon Ellis (2021). *skimr: Compact and Flexible Summaries of Data*. R package version 2.1.3. https://CRAN.R-project.org/package=skimr
  
* H. Wickham. *ggplot2: Elegant Graphics for Data Analysis*. Springer-Verlag New York, 2016.
  
* Hadley Wickham, Romain François, Lionel Henry and Kirill Müller (2021). *dplyr: A Grammar of Data Manipulation*. R package version 1.0.5. https://CRAN.R-project.org/package=dplyr

* Hlavac, Marek (2018). *stargazer: Well-Formatted Regression and Summary Statistics Tables*. R package version 5.2.1. https://CRAN.R-project.org/package=stargazer

* Kranz, S. (2020). *RTutor. Creating R problem sets with automatic assement of student's solutions*. R package version 2020.11.25 https://github.com/skranz/RTutor  

* Winston Chang (2019). *webshot: Take Screenshots of Web Pages*. R package version 0.5.2. https://CRAN.R-project.org/package=webshot
  
